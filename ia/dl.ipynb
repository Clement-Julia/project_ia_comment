{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning MBIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import load_model\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Désactiver les avertissements\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    doc = nlp(text, disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])\n",
    "    simplified_text = \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_stop and not token.is_digit])\n",
    "    \n",
    "    return simplified_text\n",
    "\n",
    "if os.path.exists(\"sentiment.keras\"):\n",
    "    model = load_model(\"sentiment.keras\")\n",
    "else:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "    data[\"review\"] = data[\"review\"].apply(preprocess_text)\n",
    "    data[\"sentiment\"] = label_encoder.fit_transform(data[\"sentiment\"])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[\"review\"], data[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    max_sequence_length = 100\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(y_train),y = y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "        LSTM(64),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "    model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "    y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "    model.save(\"sentiment.keras\")\n",
    "\n",
    "comment = \"You are so beautiful !\"\n",
    "comment = preprocess_text(comment)\n",
    "# comment = GoogleTranslator(source='auto', target='en').translate(comment)\n",
    "comment_sequence = tokenizer.texts_to_sequences([comment])\n",
    "comment_padded = pad_sequences(comment_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "predictions = model.predict(comment_padded)\n",
    "class_labels = ['Négatif', 'Positif', 'Neutre']\n",
    "predicted_class = class_labels[np.argmax(predictions)]\n",
    "print(predictions)\n",
    "print(f\"Première prédiction classé comme: {predicted_class}\")\n",
    "\n",
    "accuracy = model.evaluate(X_test_padded, y_test_encoded)[1]\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Toxic Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import load_model\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Désactiver les avertissements\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text\n",
    "    # text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    # doc = nlp(text, disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])\n",
    "    # simplified_text = \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_stop and not token.is_digit])\n",
    "    \n",
    "    return simplified_text\n",
    "\n",
    "def clear_data(data, var):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    data_copy[\"comment_text\"] = data_copy[\"comment_text\"].astype(str)\n",
    "    data_copy = data_copy.drop_duplicates(subset=\"comment_text\")\n",
    "    data_copy[\"comment_text\"] = data_copy[\"comment_text\"].dropna()\n",
    "\n",
    "    toxic_samples = data_copy[data_copy[var] == 1].drop_duplicates(subset=\"comment_text\")\n",
    "    non_toxic_samples = data_copy[data_copy[var] == 0].drop_duplicates(subset=\"comment_text\")\n",
    "    num_toxic_samples = min(50000, len(toxic_samples))\n",
    "    toxic_samples = toxic_samples.sample(n=num_toxic_samples, random_state=42)\n",
    "    non_toxic_samples = non_toxic_samples.sample(n=num_toxic_samples, random_state=42)\n",
    "    new_data = pd.concat([toxic_samples, non_toxic_samples])\n",
    "\n",
    "    data = new_data.sample(frac=1, random_state=42)\n",
    "    data[\"comment_text\"] = data[\"comment_text\"].apply(preprocess_text)\n",
    "\n",
    "    return data\n",
    "\n",
    "if os.path.exists(\"toxic.keras\"):\n",
    "    model = load_model(\"toxic.keras\")\n",
    "else:\n",
    "    data = pd.read_csv(\"../data/train_1M.csv\")\n",
    "    data = clear_data(data, \"toxic\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[\"comment_text\"], data[\"toxic\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    max_sequence_length = 100\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "        LSTM(64),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "    model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "    y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "    model.save(\"toxic.keras\")\n",
    "\n",
    "comment = \"You are so beautiful !\"\n",
    "comment = preprocess_text(comment)\n",
    "# comment = GoogleTranslator(source='auto', target='en').translate(comment)\n",
    "comment_sequence = tokenizer.texts_to_sequences([comment])\n",
    "comment_padded = pad_sequences(comment_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "predictions = model.predict(comment_padded)\n",
    "class_labels = ['Négatif', 'Positif', 'Neutre']\n",
    "predicted_class = class_labels[np.argmax(predictions)]\n",
    "# print(predictions)\n",
    "print(f\"Commentaire classé comme: {predicted_class}\")\n",
    "\n",
    "# accuracy = model.evaluate(X_test_padded, y_test_encoded)[1]\n",
    "# print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import load_model\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Désactiver les avertissements\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer()\n",
    "variables = [\"toxic\", \"obscene\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # return text\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    doc = nlp(text, disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "    simplified_text = \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_stop and not token.is_digit])\n",
    "    \n",
    "    return simplified_text\n",
    "\n",
    "def clear_data(data, var):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    data_copy[\"comment_text\"] = data_copy[\"comment_text\"].astype(str)\n",
    "    data_copy = data_copy.drop_duplicates(subset=\"comment_text\")\n",
    "    data_copy[\"comment_text\"] = data_copy[\"comment_text\"].dropna()\n",
    "\n",
    "    toxic_samples = data_copy[data_copy[var] == 1].drop_duplicates(subset=\"comment_text\")\n",
    "    non_toxic_samples = data_copy[data_copy[var] == 0].drop_duplicates(subset=\"comment_text\")\n",
    "    num_toxic_samples = min(50000, len(toxic_samples))\n",
    "    toxic_samples = toxic_samples.sample(n=num_toxic_samples, random_state=42)\n",
    "    non_toxic_samples = non_toxic_samples.sample(n=num_toxic_samples, random_state=42)\n",
    "    data = pd.concat([toxic_samples, non_toxic_samples])\n",
    "    # print(data)\n",
    "\n",
    "    data[\"comment_text\"] = data[\"comment_text\"].apply(preprocess_text)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for toxic_var in variables:\n",
    "    if os.path.exists(f\"{toxic_var}.keras\"):\n",
    "        model = load_model(f\"{toxic_var}.keras\")\n",
    "    else:\n",
    "        data = pd.read_csv(\"../data/train_1M.csv\")\n",
    "        data = clear_data(data, toxic_var)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[\"comment_text\"], data[toxic_var], test_size=0.2, random_state=42)\n",
    "\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "        max_sequence_length = 100\n",
    "        X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "        X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "            LSTM(64),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "        y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "        model.fit(X_train_padded, y_train_encoded, epochs=15, batch_size=32, validation_split=0.1)\n",
    "\n",
    "        y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "        model.save(f\"{toxic_var}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../static/model/toxic.keras\")\n",
    "comment = \"Wow\"\n",
    "comment = preprocess_text(comment)\n",
    "# comment = GoogleTranslator(source='auto', target='en').translate(comment)\n",
    "comment_sequence = tokenizer.texts_to_sequences([comment])\n",
    "comment_padded = pad_sequences(comment_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "predictions = model.predict(comment_padded)\n",
    "class_labels = ['Négatif', 'Positif', 'Neutre']\n",
    "predicted_class = class_labels[np.argmax(predictions)]\n",
    "# print(predictions)\n",
    "print(f\"Commentaire classé comme: {predicted_class}\")\n",
    "\n",
    "# accuracy = model.evaluate(X_test_padded, y_test_encoded)[1]\n",
    "# print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
