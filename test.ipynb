{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kirit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "c:\\Users\\kirit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages:\n",
      "severe_toxicity: 0.62%\n",
      "obscene: 2.58%\n",
      "sexual_explicit: 0.83%\n",
      "identity_attack: 1.87%\n",
      "insult: 11.58%\n",
      "threat: 0.84%\n",
      "Total Percentage: 3.05%\n",
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from deep_translator import GoogleTranslator\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "data = pd.read_csv(\"data/train_1M.csv\")\n",
    "data = data.head(100000)\n",
    "data = data.drop_duplicates(subset=\"comment_text\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text,disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])\n",
    "    simplified_text = \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_stop and not token.is_digit])\n",
    "\n",
    "    return simplified_text\n",
    "    # return text\n",
    "\n",
    "data[\"comment_text\"] = data[\"comment_text\"].fillna(\"\").apply(preprocess_text)\n",
    "\n",
    "def classify_toxicity(toxicity_score, threshold=0.5):\n",
    "    if toxicity_score > threshold:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "\n",
    "data['sentiment'] = data['toxicity'].apply(classify_toxicity)\n",
    "# le = LabelEncoder()\n",
    "# data['sentiment'] = le.fit_transform(data['sentiment'])\n",
    "\n",
    "def analyze_comment(comment):\n",
    "    comment = GoogleTranslator(source='auto', target='en').translate(comment)\n",
    "    comment = preprocess_text(comment)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    classifier = MultinomialNB()\n",
    "\n",
    "    X_train_vectors = vectorizer.fit_transform(data[\"comment_text\"])\n",
    "    classifier.fit(X_train_vectors, data[\"sentiment\"])\n",
    "\n",
    "    comment_vector = vectorizer.transform([comment])\n",
    "    prediction = classifier.predict(comment_vector)\n",
    "\n",
    "    percentages = {}\n",
    "    for column in data.columns[14:20]:\n",
    "        percentages[column] = data[column].mean() * 100\n",
    "\n",
    "    total_percentage = np.mean(list(percentages.values()))\n",
    "\n",
    "    sentiment = prediction[0]\n",
    "\n",
    "    result = f\"Percentages:\\n\"\n",
    "    for column, percentage in percentages.items():\n",
    "        result += f\"{column}: {percentage:.2f}%\\n\"\n",
    "    result += f\"Total Percentage: {total_percentage:.2f}%\\n\"\n",
    "    result += f\"Sentiment: {sentiment}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "comment = \"You are ugly\"\n",
    "result = analyze_comment(comment)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from deep_translator import GoogleTranslator\n",
    "# import spacy\n",
    "# import numpy as np\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Charger les données d'apprentissage\n",
    "# data = pd.read_csv(\"data/train_1M.csv\")\n",
    "# data = data.head(5000)\n",
    "# data = data.drop_duplicates(subset=\"comment_text\")\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     simplified_text = \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_stop and not token.is_digit])\n",
    "\n",
    "#     return simplified_text\n",
    "\n",
    "# data[\"comment_text\"] = data[\"comment_text\"].fillna(\"\").apply(preprocess_text)\n",
    "\n",
    "# def classify_toxicity(toxicity_score, threshold=0.5):\n",
    "#     if toxicity_score > threshold:\n",
    "#         return 'negative'\n",
    "#     else:\n",
    "#         return 'positive'\n",
    "\n",
    "# data['sentiment'] = data['toxicity'].apply(classify_toxicity)\n",
    "# data = data.dropna(subset=[\"sentiment\"])\n",
    "\n",
    "# # Entraîner le modèle et créer un vecteur TF-IDF\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# classifier = MultinomialNB()\n",
    "\n",
    "# X_train_vectors = vectorizer.fit_transform(data[\"comment_text\"])\n",
    "# classifier.fit(X_train_vectors, data[\"sentiment\"])\n",
    "\n",
    "# # Sauvegarder le modèle, le vecteur TF-IDF et les pourcentages\n",
    "# model_path = \"sentiment_model.pkl\"\n",
    "# vectorizer_path = \"tfidf_vectorizer.pkl\"\n",
    "\n",
    "# import joblib\n",
    "\n",
    "# joblib.dump(classifier, model_path)\n",
    "# joblib.dump(vectorizer, vectorizer_path)\n",
    "\n",
    "# percentages = {}\n",
    "# for column in data.columns[7:16]:\n",
    "#     percentages[column] = data[column].mean() * 100\n",
    "\n",
    "# joblib.dump(percentages, \"percentages.pkl\")\n",
    "\n",
    "# # Charger le modèle, le vecteur TF-IDF et les pourcentages lors de l'analyse d'un nouveau commentaire\n",
    "# def analyze_comment(comment):\n",
    "#     comment = GoogleTranslator(source='auto', target='en').translate(comment)\n",
    "#     comment = preprocess_text(comment)\n",
    "\n",
    "#     loaded_model = joblib.load(model_path)\n",
    "#     loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "#     comment_vector = loaded_vectorizer.transform([comment])\n",
    "#     prediction = loaded_model.predict(comment_vector)\n",
    "\n",
    "#     loaded_percentages = joblib.load(\"percentages.pkl\")\n",
    "#     total_percentage = np.mean(list(loaded_percentages.values()))\n",
    "\n",
    "#     sentiment = prediction[0]\n",
    "\n",
    "#     result = f\"Percentages:\\n\"\n",
    "#     for column, percentage in loaded_percentages.items():\n",
    "#         result += f\"{column}: {percentage:.2f}%\\n\"\n",
    "#     result += f\"Total Percentage: {total_percentage:.2f}%\\n\"\n",
    "#     result += f\"Sentiment: {sentiment}\"\n",
    "\n",
    "#     return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
